# ì£¼ìš” ë…¼ë¬¸ ëª©ë¡ ë° ë§í¬

**Topic**: AI-Modeling-Transformer
**ìƒì„±ì¼**: 2026-01-10

---

## ğŸ“‹ ëª¨ë“ˆë³„ í•„ìˆ˜ ë…¼ë¬¸

### M1 - Transformer íƒ„ìƒê³¼ ê¸°ë³¸ ì›ë¦¬

#### 1. Attention is All You Need (2017) â­â­â­
- **ì €ì**: Vaswani et al., Google
- **ë°œí‘œ**: NIPS 2017
- **ArXiv**: https://arxiv.org/abs/1706.03762
- **í•µì‹¬ ë‚´ìš©**: Transformer ì•„í‚¤í…ì²˜ ìµœì´ˆ ì œì•ˆ, Self-Attention ë©”ì»¤ë‹ˆì¦˜
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ìµœìš°ì„  (M1 í•µì‹¬)
- **ë‹¤ìš´ë¡œë“œ**: PDF ì €ì¥ ê¶Œì¥

#### 2. Improving Language Understanding by Generative Pre-Training (GPT-1, 2018)
- **ì €ì**: Radford et al., OpenAI
- **ë°œí‘œ**: OpenAI Blog
- **ë§í¬**: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- **í•µì‹¬ ë‚´ìš©**: Transformer Decoder ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸, Pre-training + Fine-tuning
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: í•„ìˆ˜ (M1/M2)

#### 3. BERT: Pre-training of Deep Bidirectional Transformers (2018)
- **ì €ì**: Devlin et al., Google
- **ë°œí‘œ**: NAACL 2019
- **ArXiv**: https://arxiv.org/abs/1810.04805
- **í•µì‹¬ ë‚´ìš©**: Bidirectional Transformer, MLM (Masked Language Modeling)
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: í•„ìˆ˜ (M1/M3)

---

### M2 - GPT ì‹œë¦¬ì¦ˆì˜ ì§„í™”

#### 4. Language Models are Unsupervised Multitask Learners (GPT-2, 2019)
- **ì €ì**: Radford et al., OpenAI
- **ë§í¬**: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
- **í•µì‹¬ ë‚´ìš©**: Zero-shot Learning, 1.5B íŒŒë¼ë¯¸í„°
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: í•„ìˆ˜

#### 5. Language Models are Few-Shot Learners (GPT-3, 2020)
- **ì €ì**: Brown et al., OpenAI
- **ë°œí‘œ**: NeurIPS 2020
- **ArXiv**: https://arxiv.org/abs/2005.14165
- **í•µì‹¬ ë‚´ìš©**: Few-shot Learning, 175B íŒŒë¼ë¯¸í„°, Scaling Laws
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ìµœìš°ì„  (M2 í•µì‹¬)

---

### M3 - Encoder-Decoder ë° ì–‘ë°©í–¥ ëª¨ë¸

#### 6. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5, 2019)
- **ì €ì**: Raffel et al., Google
- **ë°œí‘œ**: JMLR 2020
- **ArXiv**: https://arxiv.org/abs/1910.10683
- **í•µì‹¬ ë‚´ìš©**: Text-to-Text Framework, Encoder-Decoder êµ¬ì¡°
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: í•„ìˆ˜

#### 7. RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019)
- **ì €ì**: Liu et al., Facebook AI
- **ArXiv**: https://arxiv.org/abs/1907.11692
- **í•µì‹¬ ë‚´ìš©**: BERT ìµœì í™” ê°œì„ 
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ê¶Œì¥ (ì‹œê°„ ìˆì„ ë•Œ)

#### 8. ALBERT: A Lite BERT for Self-supervised Learning (2019)
- **ì €ì**: Lan et al., Google
- **ArXiv**: https://arxiv.org/abs/1909.11942
- **í•µì‹¬ ë‚´ìš©**: íŒŒë¼ë¯¸í„° ê³µìœ , ê²½ëŸ‰í™”
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ì„ íƒ

---

### M4 - ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ìµœì‹  ê¸°ë²•

#### 9. Training language models to follow instructions with human feedback (InstructGPT, 2022)
- **ì €ì**: Ouyang et al., OpenAI
- **ArXiv**: https://arxiv.org/abs/2203.02155
- **í•µì‹¬ ë‚´ìš©**: RLHF (Reinforcement Learning from Human Feedback)
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ìµœìš°ì„  (M4 í•µì‹¬)

#### 10. GPT-4 Technical Report (2023)
- **ì €ì**: OpenAI
- **ArXiv**: https://arxiv.org/abs/2303.08774
- **í•µì‹¬ ë‚´ìš©**: Multimodal, ì„±ëŠ¥ í–¥ìƒ
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: í•„ìˆ˜

#### 11. LLaMA: Open and Efficient Foundation Language Models (2023)
- **ì €ì**: Touvron et al., Meta AI
- **ArXiv**: https://arxiv.org/abs/2302.13971
- **í•µì‹¬ ë‚´ìš©**: ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ê·œëª¨ ëª¨ë¸, 7B-65B
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: í•„ìˆ˜

#### 12. Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)
- **ì €ì**: Touvron et al., Meta AI
- **ArXiv**: https://arxiv.org/abs/2307.09288
- **í•µì‹¬ ë‚´ìš©**: LLaMA ê°œì„ , ì±—ë´‡ ìµœì í™”
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ê¶Œì¥

---

### M5 - ìµœì‹  íŠ¸ë Œë“œì™€ ë¯¸ë˜ ë°©í–¥

#### 13. Gemini: A Family of Highly Capable Multimodal Models (2023)
- **ì €ì**: Google DeepMind
- **ArXiv**: https://arxiv.org/abs/2312.11805
- **í•µì‹¬ ë‚´ìš©**: Multimodal, Ultra/Pro/Nano ì‹œë¦¬ì¦ˆ
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ê¶Œì¥

#### 14. Claude 3 Model Card (2024)
- **ì €ì**: Anthropic
- **ë§í¬**: https://www.anthropic.com/news/claude-3-family
- **í•µì‹¬ ë‚´ìš©**: Opus/Sonnet/Haiku, Constitutional AI
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ê¶Œì¥ (ê³µì‹ ë¸”ë¡œê·¸)

---

## ğŸ“š ì¶”ê°€ ì½ê¸° (ì„ íƒ)

### Attention ë©”ì»¤ë‹ˆì¦˜ ì´ì „ (ì—­ì‚¬ì  ë§¥ë½)

#### 15. Neural Machine Translation by Jointly Learning to Align and Translate (2014)
- **ì €ì**: Bahdanau et al.
- **ArXiv**: https://arxiv.org/abs/1409.0473
- **í•µì‹¬ ë‚´ìš©**: Attention ë©”ì»¤ë‹ˆì¦˜ ìµœì´ˆ ì œì•ˆ (RNN ê¸°ë°˜)
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ì„ íƒ (ì—­ì‚¬ ì´í•´ìš©)

#### 16. Sequence to Sequence Learning with Neural Networks (2014)
- **ì €ì**: Sutskever et al., Google
- **ArXiv**: https://arxiv.org/abs/1409.3215
- **í•µì‹¬ ë‚´ìš©**: Seq2Seq ì•„í‚¤í…ì²˜
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ì„ íƒ

### ìµœì í™” ê¸°ë²•

#### 17. LoRA: Low-Rank Adaptation of Large Language Models (2021)
- **ì €ì**: Hu et al., Microsoft
- **ArXiv**: https://arxiv.org/abs/2106.09685
- **í•µì‹¬ ë‚´ìš©**: íš¨ìœ¨ì  Fine-tuning
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ì„ íƒ (M5)

#### 18. QLoRA: Efficient Finetuning of Quantized LLMs (2023)
- **ì €ì**: Dettmers et al.
- **ArXiv**: https://arxiv.org/abs/2305.14314
- **í•µì‹¬ ë‚´ìš©**: Quantization + LoRA
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ì„ íƒ

### Mixture of Experts (MoE)

#### 19. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)
- **ì €ì**: Shazeer et al., Google
- **ArXiv**: https://arxiv.org/abs/1701.06538
- **í•µì‹¬ ë‚´ìš©**: MoE ê¸°ë³¸ ê°œë…
- **ì½ê¸° ìš°ì„ ìˆœìœ„**: ê¶Œì¥ (M4)

---

## ğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ë°©ë²•

### ArXivì—ì„œ ë‹¤ìš´ë¡œë“œ
1. ArXiv ë§í¬ í´ë¦­
2. ìš°ì¸¡ "Download PDF" í´ë¦­
3. `vl_materials/papers/` í´ë”ì— ì €ì¥ (í´ë” ì§ì ‘ ìƒì„±)
4. íŒŒì¼ëª… ì˜ˆì‹œ: `Attention_is_All_You_Need_2017.pdf`

### ëª…ëª… ê·œì¹™
```
{ë…¼ë¬¸ëª…}_{ì—°ë„}.pdf
ì˜ˆ: GPT3_Language_Models_are_Few_Shot_Learners_2020.pdf
```

---

## ğŸ“– ë…¼ë¬¸ ì½ê¸° ì „ëµ

### íš¨ìœ¨ì  ì½ê¸° ìˆœì„œ
1. **Abstract**: 1-2ë¶„, í•µì‹¬ ì•„ì´ë””ì–´ íŒŒì•…
2. **Introduction**: 5ë¶„, ë°°ê²½ ë° ë™ê¸°
3. **Conclusion**: 3ë¶„, ì£¼ìš” ê²°ê³¼
4. **Figures/Tables**: 5ë¶„, ì‹œê°ì  ì´í•´
5. **Method**: 30ë¶„-1ì‹œê°„, ìƒì„¸ ë‚´ìš©

### ë©”ëª¨ ë°©ë²•
- ê° ë…¼ë¬¸ë§ˆë‹¤ 1-2í˜ì´ì§€ ìš”ì•½ ì‘ì„±
- í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬
- ì´í•´ ì•ˆ ë˜ëŠ” ë¶€ë¶„ì€ ë¸”ë¡œê·¸/ìœ íŠœë¸Œ ì°¸ê³ 
- íƒ€ì„ë¼ì¸ì— ì¶”ê°€í•  ë‚´ìš© í‘œì‹œ

---

## ğŸ”— ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤

### ë…¼ë¬¸ ê²€ìƒ‰
- **ArXiv**: https://arxiv.org/ (ìµœì‹  ë…¼ë¬¸)
- **Papers with Code**: https://paperswithcode.com/ (ì½”ë“œ í¬í•¨)
- **Google Scholar**: https://scholar.google.com/ (ì¸ìš© ì •ë³´)

### ë…¼ë¬¸ ê´€ë¦¬
- **Zotero**: ë¬´ë£Œ ë…¼ë¬¸ ê´€ë¦¬ ë„êµ¬
- **Notion**: ë…¼ë¬¸ ìš”ì•½ ì •ë¦¬
- **Obsidian**: ì—°ê²°ëœ ë…¸íŠ¸ ì‘ì„±

---

**ì‘ì„±ì**: CUA_VL Claude Skills í•™ìŠµ
**ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-10
**ì´ ë…¼ë¬¸ ìˆ˜**: 19ê°œ (í•„ìˆ˜ 14ê°œ, ì„ íƒ 5ê°œ)
